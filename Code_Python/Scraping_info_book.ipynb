{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas\n",
    "import urllib\n",
    "import csv\n",
    "import re\n",
    "import urllib\n",
    "import bs4\n",
    "\n",
    "from urllib import request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_All_Book=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Data_Traitee/Books_v4.csv\",sep=\";\",index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_Book_needed=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Data_Traitee/Books_6508.csv\",sep=\",\",index_col=None)\n",
    "df_Book_needed=df_Book_needed[[\"ISBN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142026, 9)\n",
      "-----------------------\n",
      "   Unnamed: 0       ISBN                                         Book.Title  \\\n",
      "0           2    2005018                                       Clara Callan   \n",
      "1           3   60973129                               Decision in Normandy   \n",
      "2           4  374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
      "3           6  399135782                             The Kitchen God's Wife   \n",
      "4           7  425176428  What If?: The World's Foremost Military Histor...   \n",
      "\n",
      "            Book.Author  Year.Of.Publication                 Publisher  \\\n",
      "0  Richard Bruce Wright                 2001     HarperFlamingo Canada   \n",
      "1          Carlo D'Este                 1991           HarperPerennial   \n",
      "2      Gina Bari Kolata                 1999      Farrar Straus Giroux   \n",
      "3               Amy Tan                 1991          Putnam Pub Group   \n",
      "4         Robert Cowley                 2000  Berkley Publishing Group   \n",
      "\n",
      "                                         Image.URL.S  \\\n",
      "0  http://images.amazon.com/images/P/0002005018.0...   \n",
      "1  http://images.amazon.com/images/P/0060973129.0...   \n",
      "2  http://images.amazon.com/images/P/0374157065.0...   \n",
      "3  http://images.amazon.com/images/P/0399135782.0...   \n",
      "4  http://images.amazon.com/images/P/0425176428.0...   \n",
      "\n",
      "                                         Image.URL.M  \\\n",
      "0  http://images.amazon.com/images/P/0002005018.0...   \n",
      "1  http://images.amazon.com/images/P/0060973129.0...   \n",
      "2  http://images.amazon.com/images/P/0374157065.0...   \n",
      "3  http://images.amazon.com/images/P/0399135782.0...   \n",
      "4  http://images.amazon.com/images/P/0425176428.0...   \n",
      "\n",
      "                                         Image.URL.L  \n",
      "0  http://images.amazon.com/images/P/0002005018.0...  \n",
      "1  http://images.amazon.com/images/P/0060973129.0...  \n",
      "2  http://images.amazon.com/images/P/0374157065.0...  \n",
      "3  http://images.amazon.com/images/P/0399135782.0...  \n",
      "4  http://images.amazon.com/images/P/0425176428.0...  \n",
      "--------------------------------------------------------------------------------------------\n",
      "(6508, 1)\n",
      "-----------------------\n",
      "         ISBN\n",
      "0  034545104X\n",
      "1   446520802\n",
      "2  038550120X\n",
      "3   425115801\n",
      "4   449006522\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(df_All_Book.shape)\n",
    "print(\"-----------------------\")\n",
    "print(df_All_Book.head())\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(df_Book_needed.shape)\n",
    "print(\"-----------------------\")\n",
    "print(df_Book_needed.head())\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_Final=pandas.merge(df_Book_needed, df_All_Book, on=['ISBN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6508, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Book.Title</th>\n",
       "      <th>Book.Author</th>\n",
       "      <th>Year.Of.Publication</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Image.URL.S</th>\n",
       "      <th>Image.URL.M</th>\n",
       "      <th>Image.URL.L</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>034545104X</td>\n",
       "      <td>2967</td>\n",
       "      <td>Flesh Tones: A Novel</td>\n",
       "      <td>M. J. Rose</td>\n",
       "      <td>2002</td>\n",
       "      <td>Ballantine Books</td>\n",
       "      <td>http://images.amazon.com/images/P/034545104X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/034545104X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/034545104X.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>446520802</td>\n",
       "      <td>11055</td>\n",
       "      <td>The Notebook</td>\n",
       "      <td>Nicholas Sparks</td>\n",
       "      <td>1996</td>\n",
       "      <td>Warner Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0446520802.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0446520802.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0446520802.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>038550120X</td>\n",
       "      <td>9296</td>\n",
       "      <td>A Painted House</td>\n",
       "      <td>JOHN GRISHAM</td>\n",
       "      <td>2001</td>\n",
       "      <td>Doubleday</td>\n",
       "      <td>http://images.amazon.com/images/P/038550120X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/038550120X.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/038550120X.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>425115801</td>\n",
       "      <td>2031</td>\n",
       "      <td>Lightning</td>\n",
       "      <td>Dean R. Koontz</td>\n",
       "      <td>1996</td>\n",
       "      <td>Berkley Publishing Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0425115801.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0425115801.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0425115801.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>449006522</td>\n",
       "      <td>228</td>\n",
       "      <td>Manhattan Hunt Club</td>\n",
       "      <td>JOHN SAUL</td>\n",
       "      <td>2002</td>\n",
       "      <td>Ballantine Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0449006522.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0449006522.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0449006522.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ISBN  Unnamed: 0            Book.Title      Book.Author  \\\n",
       "0  034545104X        2967  Flesh Tones: A Novel       M. J. Rose   \n",
       "1   446520802       11055          The Notebook  Nicholas Sparks   \n",
       "2  038550120X        9296       A Painted House     JOHN GRISHAM   \n",
       "3   425115801        2031             Lightning   Dean R. Koontz   \n",
       "4   449006522         228   Manhattan Hunt Club        JOHN SAUL   \n",
       "\n",
       "   Year.Of.Publication                 Publisher  \\\n",
       "0                 2002          Ballantine Books   \n",
       "1                 1996              Warner Books   \n",
       "2                 2001                 Doubleday   \n",
       "3                 1996  Berkley Publishing Group   \n",
       "4                 2002          Ballantine Books   \n",
       "\n",
       "                                         Image.URL.S  \\\n",
       "0  http://images.amazon.com/images/P/034545104X.0...   \n",
       "1  http://images.amazon.com/images/P/0446520802.0...   \n",
       "2  http://images.amazon.com/images/P/038550120X.0...   \n",
       "3  http://images.amazon.com/images/P/0425115801.0...   \n",
       "4  http://images.amazon.com/images/P/0449006522.0...   \n",
       "\n",
       "                                         Image.URL.M  \\\n",
       "0  http://images.amazon.com/images/P/034545104X.0...   \n",
       "1  http://images.amazon.com/images/P/0446520802.0...   \n",
       "2  http://images.amazon.com/images/P/038550120X.0...   \n",
       "3  http://images.amazon.com/images/P/0425115801.0...   \n",
       "4  http://images.amazon.com/images/P/0449006522.0...   \n",
       "\n",
       "                                         Image.URL.L  \n",
       "0  http://images.amazon.com/images/P/034545104X.0...  \n",
       "1  http://images.amazon.com/images/P/0446520802.0...  \n",
       "2  http://images.amazon.com/images/P/038550120X.0...  \n",
       "3  http://images.amazon.com/images/P/0425115801.0...  \n",
       "4  http://images.amazon.com/images/P/0449006522.0...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_Final.shape)\n",
    "df_Final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tous les livres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6508, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme il y a 6508 livre, on divise la base en 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Liste_df=[df_Final[0:1000],df_Final[1000:2000],df_Final[2000:3000],df_Final[3000:4000],df_Final[4000:5000],df_Final[5000:6000],df_Final[6000:]]\n",
    "Liste_book=[0,1,0,0,0,0,0]\n",
    "Liste_df_book0=list()\n",
    "Liste_df_book1=list()\n",
    "Liste_df_book3=list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Livre 1 à 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z=0\n",
    "df=Liste_df[Z]\n",
    "l=[]\n",
    "l2=[]\n",
    "book=Liste_book[Z]\n",
    "for i in range(df.shape[0]):\n",
    "    book=book+1\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    l2.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN\"][df.index[i]])\n",
    "    if(len(ISBN)<10):\n",
    "        while(len(ISBN)<10):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookfinder.com/search/?author=&title=&lang=en&isbn=\" + str(ISBN) +\"&new_used=*&destination=fr&currency=EUR&mode=basic&st=sr&ac=qr\"         \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "           \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'author'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "  \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'publisher'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"link\", {'itemprop' : 'bookformat'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "\n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'inLanguage'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('Language_','')\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    k=0\n",
    "    for link in soup.find(\"h1\"):\n",
    "        h=''\n",
    "        if(k/2 == 0):\n",
    "            l2.append(str(link)[:13])\n",
    "        else :\n",
    "            h=str(link)\n",
    "            h=h.replace('<span itemprop=\"isbn\">','')\n",
    "            h=h.replace('</span>','')\n",
    "            l2.append(h)\n",
    "        k=k+1        \n",
    "        \n",
    "    h=''    \n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-average text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE') \n",
    "        \n",
    "        \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-provider text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE')\n",
    "        \n",
    "            \n",
    "Liste_book.append(book)\n",
    "\n",
    "            \n",
    "m=[]\n",
    "m = [ [l[j], l[j+1], l[j+2],l[j+3],l[j+4]]  for j in range(0, int(len(l)), 5) ]\n",
    "df_book0 = pandas.DataFrame(index = range(len(m)), columns = [\"ISBN\",\"AUTEUR\", \"PUBLISHER\",\"EDITION\",\"LANGUAGE\"])\n",
    "for i in range(len(m)):\n",
    "    for variable in range(len(df_book0.columns)):\n",
    "        df_book0.at[i, df_book0.columns[variable]] = m[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_1_1000..csv\"\n",
    "df_book0.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book0.append(df_book0)\n",
    "\n",
    "m2=[]\n",
    "m2 = [ [l2[j], l2[j+1], l2[j+2],l2[j+3],l2[j+4]]  for j in range(0, int(len(l2)), 5) ]\n",
    "df_book1 = pandas.DataFrame(index = range(len(m2)), columns =[\"ISBN\",\"ISBN13\",\"ISBN10\", \"Avg_Rating\",\"Nb_Ratings\"])\n",
    "for i in range(len(m2)):\n",
    "    for variable in range(len(df_book1.columns)):\n",
    "        df_book1.at[i, df_book1.columns[variable]] = m2[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_1_1000.csv\"\n",
    "df_book1.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book1.append(df_book1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_book1\n",
    "l=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN13\"][df.index[i]])\n",
    "    if(len(ISBN)<13):\n",
    "        while(len(ISBN)<13):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookdepository.com/book/\" + str(ISBN) \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "    \n",
    "    add=''\n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"numberOfPages\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    add=''    \n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"datePublished\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    add=''\n",
    "    for link in soup.findAll(\"div\", {'class' : 'item-tools'}):\n",
    "        for t in link.findAll(\"span\", {'class' : 'sale-price'}):\n",
    "            add=t.get_text()\n",
    "    for link in soup.findAll(\"p\", {'class' : 'list-price'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('\\n','')\n",
    "            h=h.replace('                                            List price: ','')  \n",
    "            if(add==''):\n",
    "                add=h\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "\n",
    "n = [ l[j:j+4]  for j in range(0, int(len(l)), 4) ]\n",
    "df_book3 = pandas.DataFrame(index = range(len(n)),columns =[\"ISBN\",\"numberOfPages\",\"datePublished\",\"Price\"])\n",
    "for i in range(len(n)):\n",
    "    for variable in range(4):\n",
    "        df_book3.at[i,df_book3.columns[variable]] = n[i][variable]\n",
    "Liste_df_book3.append(df_book3)\n",
    "\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_1_1000.csv\"\n",
    "df_book3.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Livre 1000 à 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "Z=1\n",
    "df=Liste_df[Z]\n",
    "l=[]\n",
    "l2=[]\n",
    "book=Liste_book[Z]\n",
    "for i in range(df.shape[0]):\n",
    "    book=book+1\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    l2.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN\"][df.index[i]])\n",
    "    if(len(ISBN)<10):\n",
    "        while(len(ISBN)<10):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookfinder.com/search/?author=&title=&lang=en&isbn=\" + str(ISBN) +\"&new_used=*&destination=fr&currency=EUR&mode=basic&st=sr&ac=qr\"         \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "           \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'author'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "  \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'publisher'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"link\", {'itemprop' : 'bookformat'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "\n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'inLanguage'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('Language_','')\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    k=0\n",
    "    for link in soup.find(\"h1\"):\n",
    "        h=''\n",
    "        if(k/2 == 0):\n",
    "            l2.append(str(link)[:13])\n",
    "        else :\n",
    "            h=str(link)\n",
    "            h=h.replace('<span itemprop=\"isbn\">','')\n",
    "            h=h.replace('</span>','')\n",
    "            l2.append(h)\n",
    "        k=k+1        \n",
    "        \n",
    "    h=''    \n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-average text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE') \n",
    "        \n",
    "        \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-provider text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE')\n",
    "        \n",
    "            \n",
    "Liste_book.append(book)\n",
    "\n",
    "            \n",
    "m=[]\n",
    "m = [ [l[j], l[j+1], l[j+2],l[j+3],l[j+4]]  for j in range(0, int(len(l)), 5) ]\n",
    "df_book0 = pandas.DataFrame(index = range(len(m)), columns = [\"ISBN\",\"AUTEUR\", \"PUBLISHER\",\"EDITION\",\"LANGUAGE\"])\n",
    "for i in range(len(m)):\n",
    "    for variable in range(len(df_book0.columns)):\n",
    "        df_book0.at[i, df_book0.columns[variable]] = m[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_2_2000.csv\"\n",
    "df_book0.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book0.append(df_book0)\n",
    "\n",
    "m2=[]\n",
    "m2 = [ [l2[j], l2[j+1], l2[j+2],l2[j+3],l2[j+4]]  for j in range(0, int(len(l2)), 5) ]\n",
    "df_book1 = pandas.DataFrame(index = range(len(m2)), columns =[\"ISBN\",\"ISBN13\",\"ISBN10\", \"Avg_Rating\",\"Nb_Ratings\"])\n",
    "for i in range(len(m2)):\n",
    "    for variable in range(len(df_book1.columns)):\n",
    "        df_book1.at[i, df_book1.columns[variable]] = m2[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_2_2000.csv\"\n",
    "df_book1.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book1.append(df_book1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_book1\n",
    "l=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN13\"][df.index[i]])\n",
    "    if(len(ISBN)<13):\n",
    "        while(len(ISBN)<13):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookdepository.com/book/\" + str(ISBN) \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "    \n",
    "    add=''\n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"numberOfPages\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    add=''    \n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"datePublished\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    add=''\n",
    "    for link in soup.findAll(\"div\", {'class' : 'item-tools'}):\n",
    "        for t in link.findAll(\"span\", {'class' : 'sale-price'}):\n",
    "            add=t.get_text()\n",
    "    for link in soup.findAll(\"p\", {'class' : 'list-price'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('\\n','')\n",
    "            h=h.replace('                                            List price: ','')  \n",
    "            if(add==''):\n",
    "                add=h\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "\n",
    "n = [ l[j:j+4]  for j in range(0, int(len(l)), 4) ]\n",
    "df_book3 = pandas.DataFrame(index = range(len(n)),columns =[\"ISBN\",\"numberOfPages\",\"datePublished\",\"Price\"])\n",
    "for i in range(len(n)):\n",
    "    for variable in range(4):\n",
    "        df_book3.at[i,df_book3.columns[variable]] = n[i][variable]\n",
    "Liste_df_book3.append(df_book3)\n",
    "\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_2_2000.csv\"\n",
    "df_book3.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Livre 2000 à 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(133437 bytes read)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\cooky\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_readall_chunked\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m                 \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\cooky\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(1725 bytes read, 117120 more expected)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8da49226c1e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.bookfinder.com/search/?author=&title=&lang=en&isbn=\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mISBN\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m\"&new_used=*&destination=fr&currency=EUR&mode=basic&st=sr&ac=qr\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mfurl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfurl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\cooky\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'read'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m        \u001b[1;31m# It's a file-type object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mmarkup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         elif len(markup) <= 256 and (\n\u001b[1;32m    193\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;34mb'<'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\cooky\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readall_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\cooky\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_readall_chunked\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_readinto_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(133437 bytes read)"
     ]
    }
   ],
   "source": [
    "Z=2\n",
    "df=Liste_df[Z]\n",
    "l=[]\n",
    "l2=[]\n",
    "book=Liste_book[Z]\n",
    "for i in range(df.shape[0]):\n",
    "    book=book+1\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    l2.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN\"][df.index[i]])\n",
    "    if(len(ISBN)<10):\n",
    "        while(len(ISBN)<10):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookfinder.com/search/?author=&title=&lang=en&isbn=\" + str(ISBN) +\"&new_used=*&destination=fr&currency=EUR&mode=basic&st=sr&ac=qr\"         \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "           \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'author'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "  \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'publisher'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"link\", {'itemprop' : 'bookformat'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "\n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'inLanguage'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('Language_','')\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    k=0\n",
    "    for link in soup.find(\"h1\"):\n",
    "        h=''\n",
    "        if(k/2 == 0):\n",
    "            l2.append(str(link)[:13])\n",
    "        else :\n",
    "            h=str(link)\n",
    "            h=h.replace('<span itemprop=\"isbn\">','')\n",
    "            h=h.replace('</span>','')\n",
    "            l2.append(h)\n",
    "        k=k+1        \n",
    "        \n",
    "    h=''    \n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-average text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE') \n",
    "        \n",
    "        \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-provider text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE')\n",
    "        \n",
    "            \n",
    "Liste_book.append(book)\n",
    "\n",
    "            \n",
    "m=[]\n",
    "m = [ [l[j], l[j+1], l[j+2],l[j+3],l[j+4]]  for j in range(0, int(len(l)), 5) ]\n",
    "df_book0 = pandas.DataFrame(index = range(len(m)), columns = [\"ISBN\",\"AUTEUR\", \"PUBLISHER\",\"EDITION\",\"LANGUAGE\"])\n",
    "for i in range(len(m)):\n",
    "    for variable in range(len(df_book0.columns)):\n",
    "        df_book0.at[i, df_book0.columns[variable]] = m[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_3_3000.csv\"\n",
    "df_book0.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book0.append(df_book0)\n",
    "\n",
    "m2=[]\n",
    "m2 = [ [l2[j], l2[j+1], l2[j+2],l2[j+3],l2[j+4]]  for j in range(0, int(len(l2)), 5) ]\n",
    "df_book1 = pandas.DataFrame(index = range(len(m2)), columns =[\"ISBN\",\"ISBN13\",\"ISBN10\", \"Avg_Rating\",\"Nb_Ratings\"])\n",
    "for i in range(len(m2)):\n",
    "    for variable in range(len(df_book1.columns)):\n",
    "        df_book1.at[i, df_book1.columns[variable]] = m2[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_3_3000.csv\"\n",
    "df_book1.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book1.append(df_book1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_book1\n",
    "l=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN13\"][df.index[i]])\n",
    "    if(len(ISBN)<13):\n",
    "        while(len(ISBN)<13):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookdepository.com/book/\" + str(ISBN) \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "    \n",
    "    add=''\n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"numberOfPages\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    add=''    \n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"datePublished\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    add=''\n",
    "    for link in soup.findAll(\"div\", {'class' : 'item-tools'}):\n",
    "        for t in link.findAll(\"span\", {'class' : 'sale-price'}):\n",
    "            add=t.get_text()\n",
    "    for link in soup.findAll(\"p\", {'class' : 'list-price'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('\\n','')\n",
    "            h=h.replace('                                            List price: ','')  \n",
    "            if(add==''):\n",
    "                add=h\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "\n",
    "n = [ l[j:j+4]  for j in range(0, int(len(l)), 4) ]\n",
    "df_book3 = pandas.DataFrame(index = range(len(n)),columns =[\"ISBN\",\"numberOfPages\",\"datePublished\",\"Price\"])\n",
    "for i in range(len(n)):\n",
    "    for variable in range(4):\n",
    "        df_book3.at[i,df_book3.columns[variable]] = n[i][variable]\n",
    "Liste_df_book3.append(df_book3)\n",
    "\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_3_3000.csv\"\n",
    "df_book3.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Livre 3000 à 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z=3\n",
    "df=Liste_df[Z]\n",
    "l=[]\n",
    "l2=[]\n",
    "book=Liste_book[Z]\n",
    "for i in range(df.shape[0]):\n",
    "    book=book+1\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    l2.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN\"][df.index[i]])\n",
    "    if(len(ISBN)<10):\n",
    "        while(len(ISBN)<10):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookfinder.com/search/?author=&title=&lang=en&isbn=\" + str(ISBN) +\"&new_used=*&destination=fr&currency=EUR&mode=basic&st=sr&ac=qr\"         \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "           \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'author'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "  \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'publisher'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"link\", {'itemprop' : 'bookformat'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "\n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'inLanguage'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('Language_','')\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    k=0\n",
    "    for link in soup.find(\"h1\"):\n",
    "        h=''\n",
    "        if(k/2 == 0):\n",
    "            l2.append(str(link)[:13])\n",
    "        else :\n",
    "            h=str(link)\n",
    "            h=h.replace('<span itemprop=\"isbn\">','')\n",
    "            h=h.replace('</span>','')\n",
    "            l2.append(h)\n",
    "        k=k+1        \n",
    "        \n",
    "    h=''    \n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-average text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE') \n",
    "        \n",
    "        \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-provider text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE')\n",
    "        \n",
    "            \n",
    "Liste_book.append(book)\n",
    "\n",
    "            \n",
    "m=[]\n",
    "m = [ [l[j], l[j+1], l[j+2],l[j+3],l[j+4]]  for j in range(0, int(len(l)), 5) ]\n",
    "df_book0 = pandas.DataFrame(index = range(len(m)), columns = [\"ISBN\",\"AUTEUR\", \"PUBLISHER\",\"EDITION\",\"LANGUAGE\"])\n",
    "for i in range(len(m)):\n",
    "    for variable in range(len(df_book0.columns)):\n",
    "        df_book0.at[i, df_book0.columns[variable]] = m[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_4_4000.csv\"\n",
    "df_book0.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book0.append(df_book0)\n",
    "\n",
    "m2=[]\n",
    "m2 = [ [l2[j], l2[j+1], l2[j+2],l2[j+3],l2[j+4]]  for j in range(0, int(len(l2)), 5) ]\n",
    "df_book1 = pandas.DataFrame(index = range(len(m2)), columns =[\"ISBN\",\"ISBN13\",\"ISBN10\", \"Avg_Rating\",\"Nb_Ratings\"])\n",
    "for i in range(len(m2)):\n",
    "    for variable in range(len(df_book1.columns)):\n",
    "        df_book1.at[i, df_book1.columns[variable]] = m2[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_4_4000.csv\"\n",
    "df_book1.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book1.append(df_book1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_book1\n",
    "l=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN13\"][df.index[i]])\n",
    "    if(len(ISBN)<13):\n",
    "        while(len(ISBN)<13):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookdepository.com/book/\" + str(ISBN) \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "    \n",
    "    add=''\n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"numberOfPages\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    add=''    \n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"datePublished\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    add=''\n",
    "    for link in soup.findAll(\"div\", {'class' : 'item-tools'}):\n",
    "        for t in link.findAll(\"span\", {'class' : 'sale-price'}):\n",
    "            add=t.get_text()\n",
    "    for link in soup.findAll(\"p\", {'class' : 'list-price'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('\\n','')\n",
    "            h=h.replace('                                            List price: ','')  \n",
    "            if(add==''):\n",
    "                add=h\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "\n",
    "n = [ l[j:j+4]  for j in range(0, int(len(l)), 4) ]\n",
    "df_book3 = pandas.DataFrame(index = range(len(n)),columns =[\"ISBN\",\"numberOfPages\",\"datePublished\",\"Price\"])\n",
    "for i in range(len(n)):\n",
    "    for variable in range(4):\n",
    "        df_book3.at[i,df_book3.columns[variable]] = n[i][variable]\n",
    "Liste_df_book3.append(df_book3)\n",
    "\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_4_4000.csv\"\n",
    "df_book3.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Livre 4000 à 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "Z=4\n",
    "df=Liste_df[Z]\n",
    "l=[]\n",
    "l2=[]\n",
    "book=0\n",
    "for i in range(df.shape[0]):\n",
    "    book=book+1\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    l2.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN\"][df.index[i]])\n",
    "    if(len(ISBN)<10):\n",
    "        while(len(ISBN)<10):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookfinder.com/search/?author=&title=&lang=en&isbn=\" + str(ISBN) +\"&new_used=*&destination=fr&currency=EUR&mode=basic&st=sr&ac=qr\"         \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "           \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'author'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "  \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'publisher'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"link\", {'itemprop' : 'bookformat'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "\n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'inLanguage'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('Language_','')\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    k=0\n",
    "    for link in soup.find(\"h1\"):\n",
    "        h=''\n",
    "        if(k/2 == 0):\n",
    "            l2.append(str(link)[:13])\n",
    "        else :\n",
    "            h=str(link)\n",
    "            h=h.replace('<span itemprop=\"isbn\">','')\n",
    "            h=h.replace('</span>','')\n",
    "            l2.append(h)\n",
    "        k=k+1        \n",
    "        \n",
    "    h=''    \n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-average text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE') \n",
    "        \n",
    "        \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-provider text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE')\n",
    "        \n",
    "            \n",
    "Liste_book[Z]=book\n",
    "\n",
    "            \n",
    "m=[]\n",
    "m = [ [l[j], l[j+1], l[j+2],l[j+3],l[j+4]]  for j in range(0, int(len(l)), 5) ]\n",
    "df_book0 = pandas.DataFrame(index = range(len(m)), columns = [\"ISBN\",\"AUTEUR\", \"PUBLISHER\",\"EDITION\",\"LANGUAGE\"])\n",
    "for i in range(len(m)):\n",
    "    for variable in range(len(df_book0.columns)):\n",
    "        df_book0.at[i, df_book0.columns[variable]] = m[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_5_5000.csv\"\n",
    "df_book0.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book0.append(df_book0)\n",
    "\n",
    "m2=[]\n",
    "m2 = [ [l2[j], l2[j+1], l2[j+2],l2[j+3],l2[j+4]]  for j in range(0, int(len(l2)), 5) ]\n",
    "df_book1 = pandas.DataFrame(index = range(len(m2)), columns =[\"ISBN\",\"ISBN13\",\"ISBN10\", \"Avg_Rating\",\"Nb_Ratings\"])\n",
    "for i in range(len(m2)):\n",
    "    for variable in range(len(df_book1.columns)):\n",
    "        df_book1.at[i, df_book1.columns[variable]] = m2[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_5_5000.csv\"\n",
    "df_book1.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book1.append(df_book1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_book1=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_5_5000.csv\",sep=\";\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_book1\n",
    "l=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN13\"][df.index[i]])\n",
    "    if(len(ISBN)<13):\n",
    "        while(len(ISBN)<13):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookdepository.com/book/\" + str(ISBN) \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "    \n",
    "    add=''\n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"numberOfPages\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    add=''    \n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"datePublished\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    add=''\n",
    "    for link in soup.findAll(\"div\", {'class' : 'item-tools'}):\n",
    "        for t in link.findAll(\"span\", {'class' : 'sale-price'}):\n",
    "            add=t.get_text()\n",
    "    for link in soup.findAll(\"p\", {'class' : 'list-price'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('\\n','')\n",
    "            h=h.replace('                                            List price: ','')  \n",
    "            if(add==''):\n",
    "                add=h\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "\n",
    "n = [ l[j:j+4]  for j in range(0, int(len(l)), 4) ]\n",
    "df_book3 = pandas.DataFrame(index = range(len(n)),columns =[\"ISBN\",\"numberOfPages\",\"datePublished\",\"Price\"])\n",
    "for i in range(len(n)):\n",
    "    for variable in range(4):\n",
    "        df_book3.at[i,df_book3.columns[variable]] = n[i][variable]\n",
    "Liste_df_book3.append(df_book3)\n",
    "\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_5_5000.csv\"\n",
    "df_book3.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Livre 5000 à 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z=5\n",
    "df=Liste_df[Z]\n",
    "l=[]\n",
    "l2=[]\n",
    "book=Liste_book[Z]\n",
    "for i in range(df.shape[0]):\n",
    "    book=book+1\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    l2.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN\"][df.index[i]])\n",
    "    if(len(ISBN)<10):\n",
    "        while(len(ISBN)<10):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookfinder.com/search/?author=&title=&lang=en&isbn=\" + str(ISBN) +\"&new_used=*&destination=fr&currency=EUR&mode=basic&st=sr&ac=qr\"         \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "           \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'author'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "  \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'publisher'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"link\", {'itemprop' : 'bookformat'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "\n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'inLanguage'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('Language_','')\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    k=0\n",
    "    for link in soup.find(\"h1\"):\n",
    "        h=''\n",
    "        if(k/2 == 0):\n",
    "            l2.append(str(link)[:13])\n",
    "        else :\n",
    "            h=str(link)\n",
    "            h=h.replace('<span itemprop=\"isbn\">','')\n",
    "            h=h.replace('</span>','')\n",
    "            l2.append(h)\n",
    "        k=k+1        \n",
    "        \n",
    "    h=''    \n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-average text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE') \n",
    "        \n",
    "        \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-provider text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE')\n",
    "        \n",
    "            \n",
    "Liste_book.append(book)\n",
    "\n",
    "            \n",
    "m=[]\n",
    "m = [ [l[j], l[j+1], l[j+2],l[j+3],l[j+4]]  for j in range(0, int(len(l)), 5) ]\n",
    "df_book0 = pandas.DataFrame(index = range(len(m)), columns = [\"ISBN\",\"AUTEUR\", \"PUBLISHER\",\"EDITION\",\"LANGUAGE\"])\n",
    "for i in range(len(m)):\n",
    "    for variable in range(len(df_book0.columns)):\n",
    "        df_book0.at[i, df_book0.columns[variable]] = m[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_6_6000.csv\"\n",
    "df_book0.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book0.append(df_book0)\n",
    "\n",
    "m2=[]\n",
    "m2 = [ [l2[j], l2[j+1], l2[j+2],l2[j+3],l2[j+4]]  for j in range(0, int(len(l2)), 5) ]\n",
    "df_book1 = pandas.DataFrame(index = range(len(m2)), columns =[\"ISBN\",\"ISBN13\",\"ISBN10\", \"Avg_Rating\",\"Nb_Ratings\"])\n",
    "for i in range(len(m2)):\n",
    "    for variable in range(len(df_book1.columns)):\n",
    "        df_book1.at[i, df_book1.columns[variable]] = m2[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_6_6000.csv\"\n",
    "df_book1.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book1.append(df_book1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_book1\n",
    "l=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN13\"][df.index[i]])\n",
    "    if(len(ISBN)<13):\n",
    "        while(len(ISBN)<13):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookdepository.com/book/\" + str(ISBN) \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "    \n",
    "    add=''\n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"numberOfPages\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    add=''    \n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"datePublished\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    add=''\n",
    "    for link in soup.findAll(\"div\", {'class' : 'item-tools'}):\n",
    "        for t in link.findAll(\"span\", {'class' : 'sale-price'}):\n",
    "            add=t.get_text()\n",
    "    for link in soup.findAll(\"p\", {'class' : 'list-price'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('\\n','')\n",
    "            h=h.replace('                                            List price: ','')  \n",
    "            if(add==''):\n",
    "                add=h\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "\n",
    "n = [ l[j:j+4]  for j in range(0, int(len(l)), 4) ]\n",
    "df_book3 = pandas.DataFrame(index = range(len(n)),columns =[\"ISBN\",\"numberOfPages\",\"datePublished\",\"Price\"])\n",
    "for i in range(len(n)):\n",
    "    for variable in range(4):\n",
    "        df_book3.at[i,df_book3.columns[variable]] = n[i][variable]\n",
    "Liste_df_book3.append(df_book3)\n",
    "\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_6_6000.csv\"\n",
    "df_book3.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Livre 6000 à ++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z=6\n",
    "df=Liste_df[Z]\n",
    "l=[]\n",
    "l2=[]\n",
    "book=Liste_book[Z]\n",
    "for i in range(df.shape[0]):\n",
    "    book=book+1\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    l2.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN\"][df.index[i]])\n",
    "    if(len(ISBN)<10):\n",
    "        while(len(ISBN)<10):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookfinder.com/search/?author=&title=&lang=en&isbn=\" + str(ISBN) +\"&new_used=*&destination=fr&currency=EUR&mode=basic&st=sr&ac=qr\"         \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "           \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'author'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "  \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'publisher'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"link\", {'itemprop' : 'bookformat'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "\n",
    "    \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'itemprop' : 'inLanguage'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('Language_','')\n",
    "    if(h!=''):\n",
    "        l.append(h)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    k=0\n",
    "    for link in soup.find(\"h1\"):\n",
    "        h=''\n",
    "        if(k/2 == 0):\n",
    "            l2.append(str(link)[:13])\n",
    "        else :\n",
    "            h=str(link)\n",
    "            h=h.replace('<span itemprop=\"isbn\">','')\n",
    "            h=h.replace('</span>','')\n",
    "            l2.append(h)\n",
    "        k=k+1        \n",
    "        \n",
    "    h=''    \n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-average text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE') \n",
    "        \n",
    "        \n",
    "    h=''\n",
    "    for link in soup.find_all(\"span\", {'class' : 'book-rating-provider text-muted'}):\n",
    "            h=link.get_text()\n",
    "    if(h!=''):\n",
    "        l2.append(h)\n",
    "    else:\n",
    "        l2.append('VIDE')\n",
    "        \n",
    "            \n",
    "Liste_book.append(book)\n",
    "\n",
    "            \n",
    "m=[]\n",
    "m = [ [l[j], l[j+1], l[j+2],l[j+3],l[j+4]]  for j in range(0, int(len(l)), 5) ]\n",
    "df_book0 = pandas.DataFrame(index = range(len(m)), columns = [\"ISBN\",\"AUTEUR\", \"PUBLISHER\",\"EDITION\",\"LANGUAGE\"])\n",
    "for i in range(len(m)):\n",
    "    for variable in range(len(df_book0.columns)):\n",
    "        df_book0.at[i, df_book0.columns[variable]] = m[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_7_7000.csv\"\n",
    "df_book0.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book0.append(df_book0)\n",
    "\n",
    "m2=[]\n",
    "m2 = [ [l2[j], l2[j+1], l2[j+2],l2[j+3],l2[j+4]]  for j in range(0, int(len(l2)), 5) ]\n",
    "df_book1 = pandas.DataFrame(index = range(len(m2)), columns =[\"ISBN\",\"ISBN13\",\"ISBN10\", \"Avg_Rating\",\"Nb_Ratings\"])\n",
    "for i in range(len(m2)):\n",
    "    for variable in range(len(df_book1.columns)):\n",
    "        df_book1.at[i, df_book1.columns[variable]] = m2[i][variable]\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_7_7000.csv\"\n",
    "df_book1.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "Liste_df_book1.append(df_book1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=df_book1\n",
    "l=[]\n",
    "\n",
    "\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    l.append(df[\"ISBN\"][df.index[i]])\n",
    "    ISBN = str(df[\"ISBN13\"][df.index[i]])\n",
    "    if(len(ISBN)<13):\n",
    "        while(len(ISBN)<13):\n",
    "            ISBN ='0'+ISBN\n",
    "    url = \"https://www.bookdepository.com/book/\" + str(ISBN) \n",
    "    furl = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(furl, 'html.parser')\n",
    "    \n",
    "    add=''\n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"numberOfPages\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "    add=''    \n",
    "    for link in soup.find_all('span'):\n",
    "        if link.get(\"itemprop\")==\"datePublished\":\n",
    "            add=link.get_text()\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "    add=''\n",
    "    for link in soup.findAll(\"div\", {'class' : 'item-tools'}):\n",
    "        for t in link.findAll(\"span\", {'class' : 'sale-price'}):\n",
    "            add=t.get_text()\n",
    "    for link in soup.findAll(\"p\", {'class' : 'list-price'}):\n",
    "            h=link.get_text()\n",
    "            h=h.replace('\\n','')\n",
    "            h=h.replace('                                            List price: ','')  \n",
    "            if(add==''):\n",
    "                add=h\n",
    "    if(add!=''):\n",
    "        l.append(add)\n",
    "    else:\n",
    "        l.append('VIDE')\n",
    "        \n",
    "        \n",
    "\n",
    "n = [ l[j:j+4]  for j in range(0, int(len(l)), 4) ]\n",
    "df_book3 = pandas.DataFrame(index = range(len(n)),columns =[\"ISBN\",\"numberOfPages\",\"datePublished\",\"Price\"])\n",
    "for i in range(len(n)):\n",
    "    for variable in range(4):\n",
    "        df_book3.at[i,df_book3.columns[variable]] = n[i][variable]\n",
    "Liste_df_book3.append(df_book3)\n",
    "\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_7_7000.csv\"\n",
    "df_book3.to_csv(path, sep=\";\", encoding=\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_book_Info0_1=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_1_1000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info0_2=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_2_2000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info0_3=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_3_3000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info0_4=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_4_4000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info0_5=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_5_5000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info0_6=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_6_6000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info0_7=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_0_7_7000.csv\",sep=\";\",index_col=None)\n",
    "frames_Info0 = [df_book_Info0_1, df_book_Info0_2, df_book_Info0_3,df_book_Info0_4,df_book_Info0_5,df_book_Info0_6,df_book_Info0_7]\n",
    "result_Info0 = pandas.concat(frames_Info0)\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Data_Traitee/result_Info0.csv\"\n",
    "result_Info0.to_csv(path, sep=\";\", encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_book_Info1_1=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_1_1000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info1_2=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_2_2000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info1_3=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_3_3000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info1_4=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_4_4000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info1_5=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_5_5000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info1_6=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_6_6000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info1_7=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_1_7_7000.csv\",sep=\";\",index_col=None)\n",
    "frames_Info1 = [df_book_Info1_1, df_book_Info1_2, df_book_Info1_3,df_book_Info1_4,df_book_Info1_5,df_book_Info1_6,df_book_Info1_7]\n",
    "result_Info1 = pandas.concat(frames_Info1)\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Data_Traitee/result_Info1.csv\"\n",
    "result_Info1.to_csv(path, sep=\";\", encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_book_Info3_1=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_1_1000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info3_2=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_2_2000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info3_3=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_3_3000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info3_4=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_4_4000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info3_5=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_5_5000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info3_6=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_6_6000.csv\",sep=\";\",index_col=None)\n",
    "df_book_Info3_7=pandas.DataFrame.from_csv(\"C:/Users/cooky/Documents/ENSAE_3A/SAND/Book_Info_3_7_7000.csv\",sep=\";\",index_col=None)\n",
    "frames_Info3 = [df_book_Info3_1, df_book_Info3_2, df_book_Info3_3,df_book_Info3_4,df_book_Info3_5,df_book_Info3_6,df_book_Info3_7]\n",
    "result_Info3 = pandas.concat(frames_Info3)\n",
    "path = \"C:/Users/cooky/Documents/ENSAE_3A/SAND/Data_Traitee/result_Info3.csv\"\n",
    "result_Info3.to_csv(path, sep=\";\", encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
